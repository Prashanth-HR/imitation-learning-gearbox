{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "% Copyright (c) 2019 Idiap Research Institute, http://idiap.ch/\n",
    " Author: Suhan Shetty (suhan.shetty@idiap.ch | suhan.shetty@epfl.ch)\n",
    " \n",
    " Reference: \"Ergodic Exploration using Tensor Train: Applications in Insertion Tasks\", Suhan Shetty,  ‪João Silvério, and Sylvain Calinon\n",
    " \n",
    " This notebook has also been tested to work with Google-Colab (https://colab.research.google.com/notebooks (Use the Github repo address of the notebook to use with Google-Colab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Dependency: https://github.com/oseledets/ttpy\n",
    "!pip3 install ttpy\n",
    "# # Note: Make sure gfortran(or any other fortran compiler) is installed in your system before executing\n",
    "# # the above package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "# adding to the system path\n",
    "sys.path.insert(0, '/home/prashanth/Thesis/Imitation-Learning/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy\n",
    "import random\n",
    "from scipy.stats import ortho_group\n",
    "import scipy.integrate as integrate\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import pyplot as plt\n",
    "import tt\n",
    "from tt import riemannian\n",
    "from tt.cross import rect_cross as tt_cross\n",
    "import time\n",
    "from tqdm import tqdm # for-loop progress bar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### learn GMM from demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "file_path = '/home/prashanth/Thesis/Imitation-Learning/Trial_Code/data/ergodic_exp/1_ee_poses.npy'\n",
    "recorded_traj = np.load(file_path, allow_pickle=True)\n",
    "recorded_pos = recorded_traj[:,:3] # only positions, remove orientation quaterrnion\n",
    "recorded_quaters = recorded_traj[:,3:]\n",
    "#recorded_traj = recorded_traj[200:400, :]\n",
    "\n",
    "# gmm = GaussianMixture(n_components=2, random_state=0, ).fit(recorded_pos)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recorded_traj.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transfer data from base farme to ee frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Robot.kdl_utils import create_pose_from_vector, create_vector_from_pose\n",
    "import PyKDL \n",
    "\n",
    "initial_ee_pose_vector = recorded_traj[0]\n",
    "initial_ee_pose = create_pose_from_vector(initial_ee_pose_vector)\n",
    "\n",
    "recorded_traj_ee = np.empty_like(recorded_traj)\n",
    "\n",
    "for i, pose_vector in enumerate(recorded_traj):\n",
    "    pose = create_pose_from_vector(pose_vector)\n",
    "    pose_ee = initial_ee_pose.Inverse() * pose\n",
    "    pose_ee_vector = create_vector_from_pose(pose_ee)\n",
    "    recorded_traj_ee[i] = pose_ee_vector\n",
    "#ee_pose_ee = ee_pose_Rot.Inverse() * ee_pose_vector[:3]\n",
    "#ee_pose_ee = ee_pose.Inverse() * ee_pose\n",
    "print('{} : {}'.format(recorded_traj_ee[0], recorded_traj_ee.shape))\n",
    "\n",
    "recorded_pos = recorded_traj_ee[:,:3] # only positions, remove orientation quaterrnion\n",
    "recorded_quaters = recorded_traj_ee[:,3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_ = np.mean(recorded_pos, axis=0)\n",
    "std_ = np.std(recorded_pos, axis=0)\n",
    "mean_, std_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = recorded_traj.shape[0]\n",
    "time_ = np.linspace(0,1,N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(time_,recorded_pos[:,0], label='x')\n",
    "plt.plot(time_,recorded_pos[:,1], label='y')\n",
    "plt.plot(time_,recorded_pos[:,2], label='z')\n",
    "plt.xlabel(\"t\")\n",
    "plt.title('Position (x,y,z)')\n",
    "plt.legend(loc='best')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "plt.plot(time_,recorded_quaters[:,0], label='x')\n",
    "plt.plot(time_,recorded_quaters[:,1], label='y')\n",
    "plt.plot(time_,recorded_quaters[:,2], label='z')\n",
    "plt.plot(time_,recorded_quaters[:,3], label='w' )\n",
    "plt.xlabel(\"t\")\n",
    "plt.title('Orientation quaternion (x,y,z,w)')\n",
    "plt.legend(loc='best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%matplotlib qt\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from pbd_plot_tools import plot_gmm3d\n",
    "\n",
    "gmm = GaussianMixture(n_components=2, random_state=0, ).fit(recorded_pos)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(projection='3d')\n",
    "X = recorded_pos[:, 0]\n",
    "Y = recorded_pos[:, 1]\n",
    "Z = recorded_pos[:, 2]\n",
    "plt.plot(X,Y,Z)\n",
    "\n",
    "# sample from learned gmm\n",
    "n_samples=100\n",
    "[sample_pos, sampled_gauss] = gmm.sample(n_samples)\n",
    "# ax.scatter(sample_pos[:,0],sample_pos[:,1],sample_pos[:,2],c='k')\n",
    "\n",
    "\n",
    "mu = gmm.means_\n",
    "cov = gmm.covariances_\n",
    "plot_gmm3d(ax,mu,cov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get task space min max range\n",
    "min_ = np.amin(recorded_pos, axis=0)\n",
    "max_ = np.amax(recorded_pos, axis=0)\n",
    "range_ = np.ptp(recorded_pos, axis=0)\n",
    "average_ = np.average(max_ - min_)\n",
    "\n",
    "Lmin = min_ * 0.9\n",
    "#Lmax = average_*np.ones(3) + min_ # 0.3\n",
    "Lmax = max_ * 1.1\n",
    "\n",
    "dL = Lmax-Lmin\n",
    "a = (Lmax-Lmin)/(max_- min_)\n",
    "A = np.diag(a)\n",
    "b = (Lmin-A @ min_)\n",
    "\n",
    "\n",
    "#X_ = np.concatenate((recorded_pos,recorded_quaters_3d),axis=1)\n",
    "X_ = recorded_pos\n",
    "\n",
    "X  = X_@A + b\n",
    "\n",
    "x = X[:,:3]\n",
    "orient_3d = X[:,3:]\n",
    "\n",
    "min_, max_, range_, average_, Lmin, Lmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recorded_pos = (max_ - min_)/(max_ - min_)*(recorded_pos - min_) + min_\n",
    "recorded_pos_norm = (recorded_pos-min_)/(max_-min_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Lmin, Lmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show the sampled positions over training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the sampled positions over training data\n",
    "fig=plt.figure()\n",
    "gmm = GaussianMixture(n_components=1, random_state=0, ).fit(recorded_pos_norm)\n",
    "# sample from learned gmm\n",
    "n_samples=100\n",
    "[sample_pos, sampled_gauss] = gmm.sample(n_samples)\n",
    "ax.scatter(sample_pos[:,0],sample_pos[:,1],sample_pos[:,2],c='k')\n",
    "\n",
    "for n in range(0,3):\n",
    "    color = np.random.rand(3)\n",
    "    plt.plot(time_,recorded_pos_norm[:,n],c=color)\n",
    "    plt.scatter(np.linspace(min(time_),max(time_),n_samples),sample_pos[:,n],c=color.reshape(1,-1),marker='d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# State space description:\n",
    "\n",
    "d = 3 # dimension of the state space (the hyperparameters are set for cases 1<d<10 and 1<K<10)\n",
    "\n",
    "# Number of basis functions in each dimension:\n",
    "K = 5 # (recommended) 5 to 10 is enough for most practical usescases. A larger K will demand noisy trajectory and it will\n",
    "# demand more accurate integration scheme for ttWt in the control loop. Larger K*d demands larger rmax_ (in the control loop)\n",
    "# which would slow down the control loop. \n",
    "\n",
    "K_all = np.array([K]*d)\n",
    "\n",
    "# State space geometry: [0,L]^d\n",
    "L = 1.0 \n",
    "L_all = np.array([L]*d) # Length of each axis\n",
    "Lmin = 0*L_all \n",
    "Lmax = L_all\n",
    "\n",
    "\n",
    "# Lmin = Lmin\n",
    "# Lmax = Lmax\n",
    "# L_all = Lmax - Lmin\n",
    "\n",
    "print(\"Number of Fourier coefficients:{}\".format(K**d))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the Fourier basis and its gradient in tensor-train format\n",
    "\\begin{align*}\n",
    "    \\phi_k(x) &= \\cos({\\frac{2 \\pi k x}{L}}), \\text { }  k \\in \\{0,\\ldots,K-1 \\}\\\\\n",
    "    \\mathbf{\\Phi_k}(\\mathbf{x}) &= \\mathbf{\\phi}_{k_1}(x_{1})\\cdots\\mathbf{\\phi}_{k_d}(x_{d}),\\\\\n",
    "        \\nabla_{i}\\mathbf{\\Phi}\\big(\\mathbf{x}(t)\\big) &= \\mathbf{\\phi}(x_{1})\\circ\\cdots \\circ \\frac{\\partial  \\mathbf{\\phi}(x_{i})}{\\partial x} \\circ \\cdots \\circ \\mathbf{\\phi}(x_{d}),\\text{ } i \\in \\{1,\\ldots,d\\}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Fourier basis in tensor-train format\n",
    "\n",
    "# Define the elmental basis vector (y is a scalar)\n",
    "Phi  = lambda y, i: np.array([np.cos(np.pi*y*k/L_all[i])for k in range(K_all[i])])\n",
    "\n",
    "\n",
    "# Define derivative of elemental basis vector phi :\n",
    "DPhi = lambda y, i: (-np.pi/L_all[i])*np.array([(k)*np.sin(np.pi*y*k/L_all[i])for k in range(K_all[i])])\n",
    "\n",
    "# The followinf function Gives the basis functions for the domain as a tensor in TT fomat and its gradient\n",
    "# as a rank-1 tensor in TT at any given point x \n",
    "def tt_phi_dphi(x):\n",
    "    d = len(x)\n",
    "    _Phi = [Phi(x[i],i).reshape(1,-1,1) for i in range(d)]\n",
    "    _ttPHI = tt.vector.from_list(_Phi)\n",
    "    _DPhi = [DPhi(x[i],i) for i in range(d)]\n",
    "    _ttDPHI = []\n",
    "    for i in range(d):\n",
    "        tmp = _Phi[:]\n",
    "        tmp[i] = _DPhi[i].reshape(1,-1,1)\n",
    "        tt_ = tt.vector.from_list(tmp)\n",
    "        _ttDPHI.append(tt_)\n",
    "    return _ttPHI, _ttDPHI    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_comp = 2\n",
    "pose_training_data = recorded_pos_norm\n",
    "\n",
    "gmm_pos = GaussianMixture(n_components=n_comp,covariance_type='full').fit(pose_training_data)\n",
    "mu_pose = gmm_pos.means_\n",
    "cov_pose = gmm_pos.covariances_\n",
    "weights = gmm_pos.weights_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datapath = 'data/ergodic_exp/'\n",
    "np.save(datapath + \"mu.npy\",mu_pose)\n",
    "np.save(datapath + \"sigma.npy\",cov_pose)\n",
    "np.save(datapath + \"c.npy\",weights)\n",
    "np.save(datapath + \"A.npy\",A)\n",
    "np.save(datapath + \"b.npy\",b)\n",
    "np.save(datapath + \"Lm.npy\",Lmax[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the reference probability distribution $P(\\mathbf{x})$\n",
    "\n",
    "By default, a random GMM is used. You can choose other distributions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Reference probability distribution (GMM) parameters\n",
    "# nmix = 3 # number of mixture components\n",
    "# # Generate randomly mixture coefficients, mean, and covariance of th GMM\n",
    "# c = np.random.uniform(0.25,0.75,nmix)\n",
    "# c = c/np.sum(c) \n",
    "# mu = np.empty([nmix,d])\n",
    "# sigma = np.empty([nmix,d,d])\n",
    "# for j in range(nmix):\n",
    "#     mu[j,:] = np.random.uniform(0.25*L,+0.75*L,d)\n",
    "#     Q = ortho_group.rvs(dim=d)\n",
    "#     s = np.random.uniform(0.01,0.02,d)\n",
    "#     S = np.diag(s)\n",
    "#     sigma[j,:,:] = Q@S@Q.T\n",
    "\n",
    "# update data\n",
    "nmix = 2\n",
    "c = weights\n",
    "mu = mu_pose\n",
    "sigma = cov_pose\n",
    "\n",
    "print(\"Mixture Coefficients, c: \",c)\n",
    "print(\"\\nMean, mu:\\n \",mu)\n",
    "\n",
    "# GMM definition (reference probability distribution)\n",
    "def p(x): # Or replace with any distribution \n",
    "    result = 0 \n",
    "    for k in range(nmix):\n",
    "        n_coef = (np.linalg.det(sigma[k,:,:])*(2*np.pi)**(d))**(-0.5)\n",
    "        l = 0.5*(x-mu[k,:])@np.linalg.inv(sigma[k,:,:])@(x-mu[k,:]).T\n",
    "        result = result + c[k]*n_coef*np.exp(-l)\n",
    "\n",
    "    #print('x:{}, result:{}'.format(x, result))\n",
    "    return result\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting (Check if the distribution is bounded well)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Discretize the statespace for plotting\n",
    "dh0 = 0.05\n",
    "xa = [np.arange(Lmin[i],Lmax[i]+dh0,dh0) for i in range(d)] # discretize each dimensions\n",
    "n = [len(xa[i]) for i in range(d)] # number of dscrete states along each dimension\n",
    "\n",
    "#print(xa)\n",
    "# Choose Axes to plot\n",
    "ix = 0\n",
    "iy = 1\n",
    "P = np.empty([n[ix],n[iy]])\n",
    "for i,x_ in enumerate(xa[ix]):\n",
    "    for j,y_ in enumerate(xa[iy]):    \n",
    "        P[i,j]=0\n",
    "        for eps in np.linspace(0,1,5):\n",
    "            mu_ = np.matmul(c,mu)\n",
    "            x = eps*L_all\n",
    "            x[ix] = x_\n",
    "            x[iy] = y_\n",
    "            #print('{},{},{}'.format(P[i,j], p(x), x))\n",
    "            P[i,j] = P[i,j]+ p(x)\n",
    "\n",
    "error = np.linalg.norm\n",
    "\n",
    "\n",
    "fig = plt.figure()\n",
    "\n",
    "xx,yy = np.meshgrid(xa[ix],xa[iy])\n",
    "\n",
    "plt.contourf(xa[ix],xa[iy],P.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Fourier coefficient of the reference distrinution $P(\\mathbf{x})$ is given by:\n",
    " $$ \\mathbf{\\hat{\\mathcal{W}}}_{\\mathbf{k}} =  \\int_{x_1=0}^L \\cdots \\int_{x_d=0}^L P(\\mathbf{x})\\mathbf{\\Phi_k}(\\mathbf{x}) dx_{1} \\ldots dx_{d}. $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find Fourier coefficients of arbitrary distribution\n",
    "Uses the result given in  Eq(7) of the paper.\n",
    "\\begin{equation}\n",
    "    \\mathbf{\\hat{\\mathcal{W}}}^m\\!(k) = \\sum_{j=1}^N \\alpha_{j} \\, \\mathbf{\\mathcal{P}}^m_{:,:,j} \\, \\phi_{k}(x_{j}), \n",
    "    \\begin{array}{l}\n",
    "    \\forall k \\in \\{1,\\ldots,K\\},\\\\ \n",
    "    \\forall m \\in \\{1,\\ldots,d\\},\n",
    "    \\end{array}\n",
    "\\end{equation}\n",
    "\n",
    "where $(\\mathbf{\\mathcal{P}}^1, \\mathbf{\\mathcal{P}}^2, \\ldots, \\mathbf{\\mathcal{P}}^d)$, are the TT cores of $\\mathbf{\\mathcal{P}}$ in its TT representation. $\\mathbf{\\mathcal{P}}$  is the tensor corresponding to discretized form of $P(\\mathbf{x})$ ( discretized at query points of Gaussian quadrature rule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the fourier coefficient for an arbitrary distribution using TT (ref [1])\n",
    "# Use Gaussian Quadrature Rule (GQR) for integration(Ref: https://austingwalters.com/gaussian-quadrature/)\n",
    "t1 = time.time()\n",
    "\n",
    "# Get the discretization points and quadrature weights for the domain [-1,1] (standard pipeline for GQR)\n",
    "N = 10 # 10 is often sufficient. Increase N, if the pdf is not smoothly varying \n",
    "x0,w0 = np.polynomial.legendre.leggauss(N)\n",
    "\n",
    "# transform the discretization points and quadrature weights to the interval [0, L0]:\n",
    "xn = 0.5*L*(x0+1.0)\n",
    "wn = w0*0.5*L\n",
    "\n",
    "def P(I): # Discretization of pdf at GQR query points for tt-cross\n",
    "    I = I.astype(int)\n",
    "    result = np.zeros(I.shape[0])\n",
    "    for m,I_ in enumerate(I):\n",
    "        x = xn[I_]\n",
    "        w = np.prod(wn[I_])\n",
    "        result[m]=w*p(x)\n",
    "    #print('x:{}, result:{}'.format(x, result))\n",
    "    return result\n",
    "\n",
    "print(\"Computing Fourier Coefficients, ttW: \")\n",
    "p_init = tt.rand(N,d,r=2)\n",
    "cond=True\n",
    "while cond:# This way we truncate the tt periodically and we observed tt-cross is a bit faster using this way\n",
    "    ttP = tt_cross.cross(P,x0=p_init,nswp=5,kickrank=1.1,eps=1E-4)\n",
    "    if (ttP-p_init).norm()<1E-4:\n",
    "        cond=False\n",
    "    p_init = 1*ttP.round(0.0001)\n",
    "    \n",
    "# Normalise approximation error, if any\n",
    "ones_ = tt.vector.from_list([np.ones(N).reshape(1,-1,1)]*d)\n",
    "residual = tt.dot(ones_,ttP)\n",
    "ttP = ttP*(1/residual) \n",
    "print(\"\\nResidual Error: \",1-residual)\n",
    "\n",
    "# The scalar fourier basis at each grid point\n",
    "phi_all = np.zeros([N,K])\n",
    "for k in range(K):\n",
    "    phi_all[:,k] = np.cos(np.pi*xn*k/L)\n",
    "\n",
    "# Determine the Fourier Coefficents in TT format (Use analytical solution from ref [1])\n",
    "P_l = tt.vector.to_list(ttP)\n",
    "W = []\n",
    "for i in range(d):\n",
    "    P_i = P_l[i]\n",
    "    W_i = np.empty([P_i.shape[0],K, P_i.shape[2]])\n",
    "    for k in range(K):\n",
    "        W_i[:,k,:] = np.sum(P_i*(phi_all[:,k].reshape(1,-1,1)),axis=1)\n",
    "    W.append(W_i) \n",
    "\n",
    "ttW_hat = tt.vector.from_list(W) # convert list of TT-cores to tensor in TT\n",
    "\n",
    "t2 = time.time()  \n",
    "print(\"\\nRank of ttW: \",ttW_hat.r)\n",
    "print(\"\\nNumber of Fourier coefficients:{}\".format(K**d))\n",
    "print(\"\\nTime taken to compute Fourier Coefficients : \",(t2-t1)/60, \"minutes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Possible reasons for nonzero residual, if any: \n",
    "- pdf has some mass outside the ergodic space: its not a problem (expected to have some residual)\n",
    "- the distribution is concentrated in a small region of the space or highly multimodal: then increase N\n",
    "    - if the pdf is concnetrated in a small region of search space, smart way to do ergodic control in such cases is to decrease the length of the boundary L small enough to contain the pdf. A good transformation is always possible to avoid such nonsmoothness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find the optimization weights\n",
    "\n",
    "Optimization weights: $\\mathbf{\\Lambda_k} = (1+\\|\\mathbf{k}\\|^2)^{-\\frac{d+1}{2}}, \\text{where } \\mathbf{k}=(k_1,\\ldots,k_d)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the optimization weights in ergodic metric using cross-approximation\n",
    "def Lambda_func(I):\n",
    "    I = I.reshape(-1,d)\n",
    "    s = (1.0+d)/2.0 \n",
    "    Vp_ = (1+np.linalg.norm(I,axis=1)**2)**(-s)\n",
    "    return Vp_\n",
    "\n",
    "print(\"Computing Optimization Weights: \")\n",
    "t1=time.time()\n",
    "Lambda_init = tt.rand(K,d,r=1)\n",
    "ttLambda = tt_cross.cross(Lambda_func,x0=Lambda_init,nswp=15, kickrank=1,eps=1E-4, verbose=True)\n",
    "t2=time.time()\n",
    "print(\"Rank of the weights: \",ttLambda.r)\n",
    "print(\"Norm of the weights: \",ttLambda.norm())\n",
    "print(\"Time taken: \", (t2-t1)/60.,\"minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used to keep the trajectory within the ergodic space [0,L]\n",
    "def pull2centre(x, alpha=50, c=L/50):\n",
    "    '''\n",
    "    c: defines the boundary at which the correction velocity to pull back the system\\\n",
    "    to ergodic space activates, smoothly\n",
    "    alpha: determines the smoothness \n",
    "    '''\n",
    "    c1 = 1*c\n",
    "    c2 = L-c1\n",
    "    weight = (np.tanh(alpha*(x-c1)))/2 + (np.tanh(alpha*(c2-x)))/2\n",
    "    dx = -np.tanh(alpha*(x-c1))/2 +(np.tanh(alpha*(c2-x)))/2\n",
    "    return weight, dx\n",
    "\n",
    "# # Visualize the pull2centre\n",
    "# id=1\n",
    "# t = np.linspace(-1,2,500)\n",
    "# x = np.random.rand(500,d)\n",
    "# x[:,id]=t\n",
    "# sq = 0*x\n",
    "# pl = 0*x\n",
    "# ct = -1\n",
    "# for x_ in x:\n",
    "#     ct+=1\n",
    "#     sq[ct,:], pl[ct,:]= pull2centre(x_)\n",
    "\n",
    "# plt.plot(t,sq[:,id],\"--b\")\n",
    "# plt.plot(t,pl[:,id],\"--r\")\n",
    "# #plt.plot(t,sq[:,id] * pl[:,id],\"--g\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimal representation of the TT\n",
    "\n",
    "# Optimally represent Fourier Coef of reference distribution in TT format\n",
    "ttW_hat = ttW_hat.round(1E-1)# 1E-1 to 1E-2 is sufficient in practice\n",
    "\n",
    "# Optimization weights in its optimal representation\n",
    "ttLambda = ttLambda.round(1E-2) # 1E-2 is sufficient in practice\n",
    "\n",
    "print(\"Rank of ttWp: {} \\nRank of ttVLambda: {} \".format(ttW_hat.r, ttLambda.r))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ergodic Control Loop\n",
    "\n",
    "\\begin{align*}\n",
    "    u_{i}(t) &= u_{\\max}\\frac{b_{i}(t)}{\\|{\\mathbf{b}}(t)\\|}, i \\in \\{1,\\ldots,d\\}\\\\\n",
    "    \\text{with}\\quad {b}_{i}(t) &= \\sum_{\\mathbf{k}\\in \\mathcal{K}}\\mathbf{\\Lambda}_{\\mathbf{k}}\\big(\\mathbf{\\mathcal{W}}_{\\mathbf{k}}(t)-\\mathbf{\\hat{\\mathcal{W}}}_{\\mathbf{k}}\\big) \\, \\nabla_{i}\\mathbf{\\Phi}_{\\mathbf{k}}\\big(\\mathbf{x}(t)\\big),\\\\\n",
    "    \\nabla_{i}\\mathbf{\\Phi}\\big(\\mathbf{x}(t)\\big) &= \\mathbf{\\phi}(x_{1})\\circ\\cdots \\circ \\frac{\\partial \\mathbf{\\phi}(x_{i})}{\\partial x} \\circ \\cdots \\circ \\mathbf{\\phi}(x_{d}).\n",
    "\\end{align*} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ergodic Control Loop\n",
    "\n",
    "umax = 0.5 # Maximum velocity of the point-mass system (umax*dt is the maximum displacement). Keep it small to avoid numerical issues\n",
    "\n",
    "x = L*np.random.rand(1,d).reshape(-1,) # initial point of the dynamical system, choose any\n",
    "T = 30 # Duration of ergodic exploration in seconds\n",
    "dt = 0.01 # Run at 100Hz\n",
    "\n",
    "rmax_ = int(d*(np.max(ttW_hat.r)+2)) # Tune this hyperparamer carefully. This is the maximal upper rank of ttWt.If this is low, \n",
    "# there might be convergence issues. If it is too high, the speed of control will be effected.\n",
    "flush_every = 5 # (tune this) Every these many iterations of control loop, TT-rounding will be applied to ttWt\n",
    "\n",
    "ttWt = 0*tt.rand(ttW_hat.n,ttW_hat.d,r=1) #Initialise time-avg stats of dynamical system's trajectory\n",
    "\n",
    "# Store the trajectory for plotting\n",
    "traj = [x] \n",
    "erg_metric = []\n",
    "\n",
    "t_ = np.arange(0,T,dt)\n",
    "ct = 0\n",
    "tavg = 0\n",
    "t_iter = tqdm(t_[1:])\n",
    "for t in t_iter:\n",
    "    t1 = time.time()\n",
    "    \n",
    "    ct +=1\n",
    "    ttPHI, ttDPHI = tt_phi_dphi(x)\n",
    "    delta = ttPHI \n",
    "    ttWt =  ttWt+delta # Euler integration. This increases the rank of ttWt \n",
    "    #ttWt_ = tt.riemannian.projector_splitting_add(ttWt_,delta) # #\n",
    "    # Ideally, the above integration should be used, but due to a bug in ttpy it does not work as expected\n",
    "    if (ct+1)%flush_every==0: #\n",
    "        # flush ttWt \n",
    "        ttWt = ttWt.round(eps=1E-4,rmax=rmax_)\n",
    "    if (ct+1)%int(100/umax) ==0: # this block is not needed if d<7\n",
    "        # flush ttWt \n",
    "        ttWt = ttWt.round(eps=1E-1,rmax=rmax_)\n",
    "        \n",
    "    ttdW = (ttWt-ttW_hat*ct) \n",
    "    ttVdW = ttdW*ttLambda\n",
    "    b = np.array([tt.dot(ttVdW,ttDPHI[i]) for i in range(d)])\n",
    "    bn = np.linalg.norm(b)+1E-10\n",
    "    erg_ctrl = -(umax/bn)*b # control input from ergodic controller(velocity)\n",
    "    # Correction velocity to keep the system in the ergodic space [0,1]x...x[0,1]\n",
    "    weight_,centre_pull = pull2centre(x,alpha=20,c=L_all/20)\n",
    "    centre_pull = umax*centre_pull/(np.linalg.norm(centre_pull)+1E-8) \n",
    "    dx = erg_ctrl*weight_ + centre_pull*(1-weight_)\n",
    "    dx = umax*dx/(np.linalg.norm(dx)+1E-8)\n",
    "    x = np.clip(x+dt*dx,Lmin,L_all)\n",
    "    if bn<1E-9:\n",
    "        print(\"Terminated at \",t)\n",
    "        break\n",
    "        \n",
    "    t2 = time.time()\n",
    "    # For plotting\n",
    "    dW = ttdW*(1/ct)\n",
    "    err_ = (ttLambda*dW).norm()\n",
    "    erg_metric.append(err_)\n",
    "    tavg = tavg + (t2-t1)\n",
    "    traj.append(x)\n",
    "\n",
    "print(\"\\nAverage time per loop: \", tavg/ct, \"sec\")\n",
    "traj = np.array(traj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traj_unnorm = (traj * (max_ - min_)) + min_\n",
    "np.save('data/ergodic_exp/trajectory.npy', traj_unnorm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.min(erg_metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(erg_metric[:-1])\n",
    "plt.xlabel(\"step count\")\n",
    "plt.ylabel(\"ergodic metric\")\n",
    "# If the loss is diverging away from zero for very large d or large K0:\n",
    "# increase rmax_ for ttWt.round() (it will slow down the control loop though)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%matplotlib qt\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "gmm = gmm_pos\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(projection='3d')\n",
    "X = traj[:, 0]\n",
    "Y = traj[:, 1]\n",
    "Z = traj[:, 2]\n",
    "plt.plot(X,Y,Z)\n",
    "\n",
    "# sample from learned gmm\n",
    "n_samples=100\n",
    "[sample_pos, sampled_gauss] = gmm.sample(n_samples)\n",
    "# ax.scatter(sample_pos[:,0],sample_pos[:,1],sample_pos[:,2],c='k')\n",
    "\n",
    "from pbd_plot_tools import plot_gmm3d\n",
    "\n",
    "# mu = gmm.means_\n",
    "# cov = gmm.covariances_\n",
    "plot_gmm3d(ax,mu,sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_traj = np.sum(traj,axis=0)/traj.shape[0]\n",
    "mean_act = np.sum(mu*c.reshape(-1,1),axis=0)\n",
    "error = np.linalg.norm(mean_traj-mean_act)\n",
    "print(\"error in expectation (if GMM was used): \",error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discretize the statespace for plotting\n",
    "dh0 = 0.05\n",
    "xa = [np.arange(Lmin[i],Lmax[i]+dh0,dh0) for i in range(d)] # discretize each dimensions\n",
    "n = [len(xa[i]) for i in range(d)] # number of dscrete states along each dimension\n",
    "\n",
    "#print(xa)\n",
    "# Choose Axes to plot\n",
    "ix = 0\n",
    "iy = 1\n",
    "P = np.empty([n[ix],n[iy]])\n",
    "for i,x_ in enumerate(xa[ix]):\n",
    "    for j,y_ in enumerate(xa[iy]):    \n",
    "        P[i,j]=0\n",
    "        for eps in np.linspace(0,1,5):\n",
    "            mu_ = np.matmul(c,mu)\n",
    "            x = eps*L_all\n",
    "            x[ix] = x_\n",
    "            x[iy] = y_\n",
    "            #print('{},{},{}'.format(P[i,j], p(x), x))\n",
    "            P[i,j] = P[i,j]+ p(x)\n",
    "\n",
    "error = np.linalg.norm\n",
    "\n",
    "\n",
    "fig = plt.figure()\n",
    "\n",
    "xx,yy = np.meshgrid(xa[ix],xa[iy])\n",
    "\n",
    "plt.contourf(xa[ix],xa[iy],P.T)\n",
    "plt.plot(traj[0:m,ix],traj[0:m,iy],'--r', alpha=0.7, lw=1, label=\"robo traj\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 4 randomly chosen coordinates\n",
    "\n",
    "plt.style.use('seaborn-white')\n",
    "fig, ax = plt.subplots(2, 2, sharey=True, sharex=True)\n",
    "fig.set_figheight(8)\n",
    "fig.set_figwidth(8)\n",
    "\n",
    "# Randomly choose axis to plot\n",
    "a = np.arange(d).tolist()\n",
    "random.shuffle(a)\n",
    "ix_ = a[:2]\n",
    "iy_ = a[2:4]\n",
    "\n",
    "\n",
    "m = -1 # number of samples from the trajectory to plot\n",
    "\n",
    "# Discretize the statespace for plotting\n",
    "dh0 = 0.05\n",
    "n_sections = 20 # number of 2D slices for contour plot (increase it for better accuracy of contour plot )\n",
    "print(\"\\nFor a good accuracy of contour plot, increase 'n_sections' or decrease 'dh0' in the current cell of the notebook\")\n",
    "#xa = np.arange(0,L+dh0,dh0)  \n",
    "xa = [np.arange(Lmin[i],Lmax[i]+dh0,dh0) for i in range(d)] # discretize each dimensions\n",
    "n = [len(xa[i]) for i in range(d)] # number of dscrete states along each dimension\n",
    "#n = [len(xa) for i in range(d)] # number of dscrete states along each dimension\n",
    "xx,yy = np.meshgrid(xa,xa)\n",
    "ctx = -1\n",
    "if d==2:\n",
    "    ix_tqdm= [0]\n",
    "    iy_ = [1]\n",
    "else:\n",
    "    ix_tqdm = tqdm(ix_)\n",
    "for ix in ix_tqdm :\n",
    "    ctx+=1\n",
    "    cty=-1\n",
    "    for iy in iy_:\n",
    "        cty+=1\n",
    "        P_ = np.empty([n[ix],n[iy]])\n",
    "        for i,x_ in enumerate(xa[ix]):\n",
    "            for j,y_ in enumerate(xa[iy]): \n",
    "                P_[i,j]=0\n",
    "                for a_ in np.linspace(0,1,n_sections): \n",
    "                    x = a_*L_all\n",
    "                    x[ix] = x_\n",
    "                    x[iy] = y_\n",
    "                    P_[i,j] = P_[i,j] + p(x)\n",
    "        \n",
    "        ax[ctx,cty].contourf(xa[ix],xa[iy],P_.T, alpha=0.5)\n",
    "        ax[ctx,cty].plot(traj[0:m,ix],traj[0:m,iy],'-k', linewidth=0.25)\n",
    "        ax[ctx,cty].set_xlabel(\"$x_{}$\".format({str(ix+1)}),fontsize=12)\n",
    "        ax[ctx,cty].set_ylabel(\"$x_{}$\".format({str(iy+1)}),fontsize=12)\n",
    "        ax[ctx,cty].set_xlim([0,L])\n",
    "        ax[ctx,cty].set_ylim([0,L])\n",
    "        ax[ctx,cty].set_aspect('equal')\n",
    "        ax[ctx,cty].set_xticks([])\n",
    "        ax[ctx,cty].set_yticks([])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparision\n",
    "\n",
    "The following cells is not part of the algorithm and just here to show the difficulty in computing \n",
    "the Fourier coefficients in the standard approach. Run it if you are curious. Caution: may hang the notebook if d is larger than 4! Also, it needs patience as it may take a long time to compute one Fourier coefficient for d>3.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # In this cell, we compute one randomly chosen Fourier coefficient (there are K0^d in total)\n",
    "# d_ = 1*d # For a d=d_\n",
    "\n",
    "# K_tmp = [np.random.randint(0,K) for i in range(d_)] # randlomly select a Fourier basis function\n",
    "# def integrand(*args):\n",
    "#     phi_K = np.prod([np.cos(np.pi*args[i]*K_tmp[i]/L) for i in range(d)])\n",
    "#     c_ = p(np.array(args))\n",
    "#     return c_*phi_K\n",
    "# t0=time.time()\n",
    "# f_coef,_= integrate.nquad(integrand, [[0,L]]*d) # compute the Fourier coefficient (alternatively use Monte Carlo Integration)\n",
    "# t1= time.time()\n",
    "# print(\"Time taken to compute one Fourier Coefficint in a naive way (1): \", (t1-t0)/60,\" minutes\")\n",
    "\n",
    "\n",
    "# # # Method 2: Integration usng MCMC\n",
    "# # import mcint\n",
    "# # def integrand(x):\n",
    "# #     phi_K = np.prod([np.cos(np.pi*x[i]*K_tmp[i]/L) for i in range(d_)])\n",
    "# #     c_ = p(np.array(x))\n",
    "# #     return c_*phi_K\n",
    "\n",
    "# # def sampler():\n",
    "# #     while True:\n",
    "# #         x_sample     = np.random.uniform(0.,1.,d_)\n",
    "# #         yield x_sample\n",
    "\n",
    "\n",
    "\n",
    "# # np.random.seed(1)\n",
    "# # t0 = time.time()\n",
    "# # result, error = mcint.integrate(integrand, sampler(), measure=1.0, n=10**5)\n",
    "# # t1=time.time()\n",
    "# # print(\"expected error in MC integration (increase n if the error is large): \",error)\n",
    "# # print(\"Time taken to compute one Fourier Coefficient in a naive way (2): \", (t1-t0)/60,\" minutes\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
